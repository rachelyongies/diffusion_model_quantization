{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc4fe887-bccd-4587-91ba-b9ce7d2d2faa",
   "metadata": {},
   "source": [
    "# Quantizing Stable Diffusion for Faster Inference\n",
    "\n",
    "In this project, we investigate how **post-training quantization (PTQ)** can make\n",
    "**Stable Diffusion v1.5** faster and more memory efficient, while keeping image\n",
    "quality acceptable for use.\n",
    "\n",
    "We follow the high-level ideas of the paper **“Q-Diffusion: Quantizing Diffusion Models”**:\n",
    "focus on the **noise-prediction UNet**, perform **timestep-aware calibration** of activations,\n",
    "and then apply **low-bit quantization** to the model. \n",
    "\n",
    "Instead of re-implementing Stable Diffusion from scratch, we build on the\n",
    "**Hugging Face Diffusers** implementation and attach **fake quantization hooks**\n",
    "around the UNet’s convolution layers. This lets us compare:\n",
    "\n",
    "- FP16 (baseline, no quantization)\n",
    "- INT8-style activation quantization\n",
    "- INT4-style activation quantization\n",
    "- A “mixed” mode (INT8 in some blocks, INT4 in others)\n",
    "\n",
    "For each configuration, we measure:\n",
    "- **Latency** (seconds per generated image)\n",
    "- **Approximate VRAM usage**\n",
    "- **A quality proxy** (CLIPScore between prompt and generated image)\n",
    "\n",
    "Finally, we model the system using an **M/M/1 queue**, using the measured\n",
    "service times per mode to predict expected response time under different\n",
    "arrival rates and to study **dynamic precision switching policies**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ac2b89-b244-4cf7-aa28-fd99fd8de324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.14\n",
      "PyTorch: 2.9.0+cpu\n",
      "CUDA available: False\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# === CELL 1: Environment & device check ===\n",
    "import torch, platform, os, time\n",
    "from pprint import pprint\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
    "    torch.backends.cuda.matmul.allow_tf32 = True\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39ba077c-7055-4a7a-b704-467db0ab7e2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\MATEO\\anaconda3\\envs\\msgai-labs\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers and GPU quantization are unavailable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# === CELL 2: Install / import core libraries (run once in the env) ===\n",
    "# If already installed in your msgai-labs env, you can skip pip installs.\n",
    "\n",
    "# %pip install --upgrade diffusers transformers accelerate safetensors\n",
    "# %pip install --upgrade scipy pillow\n",
    "\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from transformers import logging as hf_logging\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict\n",
    "from contextlib import contextmanager\n",
    "from typing import Dict, List, Tuple\n",
    "import gc\n",
    "\n",
    "hf_logging.set_verbosity_error()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa37273f-5e7b-4e44-8fce-70eb7336dc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff78fde61dcb4ee5a5da5cf590c6ae11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MATEO\\anaconda3\\envs\\msgai-labs\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MATEO\\.cache\\huggingface\\hub\\models--runwayml--stable-diffusion-v1-5. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bd5a1b67ba4a959b1099debd568e2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 14 files:   0%|          | 0/14 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e5aaed24bc34ece8f5c4a456b6dddba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b219762e81e413a9692cb7e54fdba1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "scheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f85785f1183f454a89e755bbf17cf8c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738e70a1d5a74ab28f6f68fa1d0b00c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59bff1cf75a84a06a45f6028ad11f453",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a68a18af75514201b88fc5a63927826b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "961d02637dc64b6bafc2f8eae3fb703a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dde43582cf8d45dd933a89662703673e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6ed62408e6444b7a07d3cbddab13f9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906c30bcaab942969f89628336697c0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b15e05f9659d4d31b6e2067d141240d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98f0017049e04cdfb1105ce440b6d0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41f9371d2ca34140a60d96e3f9599d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "diffusion_pytorch_model.safetensors:   0%|          | 0.00/335M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# === CELL 3: Load Stable Diffusion v1.5 (baseline FP16) ===\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# Make sure you have access to this model on Hugging Face.\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# If needed: huggingface-cli login  (and/or set HF_TOKEN in env)\u001b[39;00m\n\u001b[32m      5\u001b[39m MODEL_ID = \u001b[33m\"\u001b[39m\u001b[33mrunwayml/stable-diffusion-v1-5\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m pipe = \u001b[43mStableDiffusionPipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mMODEL_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat16\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_available\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43msafety_checker\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m   \u001b[49m\u001b[38;5;66;43;03m# disable for speed/academic use\u001b[39;49;00m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m pipe = pipe.to(device)\n\u001b[32m     13\u001b[39m pipe.enable_attention_slicing(\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# slight VRAM reduction\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\msgai-labs\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\msgai-labs\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:833\u001b[39m, in \u001b[36mDiffusionPipeline.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[39m\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path.count(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m) > \u001b[32m1\u001b[39m:\n\u001b[32m    829\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    830\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mThe provided pretrained_model_name_or_path \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    831\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m is neither a valid local path nor a valid repo id. Please check the parameter.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m     cached_folder = \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    835\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    836\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    837\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    838\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    839\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    840\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    841\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    842\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_onnx\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_onnx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcustom_revision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcustom_revision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdduf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdduf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m        \u001b[49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_connected_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    850\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    852\u001b[39m     cached_folder = pretrained_model_name_or_path\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\msgai-labs\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\msgai-labs\\Lib\\site-packages\\diffusers\\pipelines\\pipeline_utils.py:1627\u001b[39m, in \u001b[36mDiffusionPipeline.download\u001b[39m\u001b[34m(cls, pretrained_model_name, **kwargs)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;66;03m# download all allow_patterns - ignore_patterns\u001b[39;00m\n\u001b[32m   1626\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1627\u001b[39m     cached_folder = \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1628\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1629\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_patterns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1636\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1637\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1639\u001b[39m     cls_name = \u001b[38;5;28mcls\u001b[39m.load_config(os.path.join(cached_folder, \u001b[33m\"\u001b[39m\u001b[33mmodel_index.json\u001b[39m\u001b[33m\"\u001b[39m)).get(\u001b[33m\"\u001b[39m\u001b[33m_class_name\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m   1640\u001b[39m     cls_name = cls_name[\u001b[32m4\u001b[39m:] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cls_name, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m cls_name.startswith(\u001b[33m\"\u001b[39m\u001b[33mFlax\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m cls_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\msgai-labs\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\msgai-labs\\Lib\\site-packages\\huggingface_hub\\_snapshot_download.py:327\u001b[39m, in \u001b[36msnapshot_download\u001b[39m\u001b[34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[39m\n\u001b[32m    325\u001b[39m         _inner_hf_hub_download(file)\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    328\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    331\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    332\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    337\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os.path.realpath(local_dir))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\msgai-labs\\Lib\\site-packages\\tqdm\\contrib\\concurrent.py:69\u001b[39m, in \u001b[36mthread_map\u001b[39m\u001b[34m(fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[33;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconcurrent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\msgai-labs\\Lib\\site-packages\\tqdm\\contrib\\concurrent.py:51\u001b[39m, in \u001b[36m_executor_map\u001b[39m\u001b[34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name=lock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,\n\u001b[32m     50\u001b[39m                       initargs=(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\msgai-labs\\Lib\\site-packages\\tqdm\\notebook.py:250\u001b[39m, in \u001b[36mtqdm_notebook.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    248\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    249\u001b[39m     it = \u001b[38;5;28msuper\u001b[39m().\u001b[34m__iter__\u001b[39m()\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mit\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# return super(tqdm...) will not catch exception\u001b[39;49;00m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[38;5;66;03m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\msgai-labs\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\msgai-labs\\Lib\\concurrent\\futures\\_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\msgai-labs\\Lib\\concurrent\\futures\\_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\msgai-labs\\Lib\\concurrent\\futures\\_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\msgai-labs\\Lib\\threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    325\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    326\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    329\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === CELL 3: Load Stable Diffusion v1.5 (baseline FP16) ===\n",
    "# Make sure you have access to this model on Hugging Face.\n",
    "# If needed: huggingface-cli login  (and/or set HF_TOKEN in env)\n",
    "\n",
    "MODEL_ID = \"runwayml/stable-diffusion-v1-5\"\n",
    "\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "    safety_checker=None,   # disable for speed/academic use\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "pipe.enable_attention_slicing(\"max\")  # slight VRAM reduction\n",
    "\n",
    "print(\"Pipeline loaded. UNet parameters:\", sum(p.numel() for p in pipe.unet.parameters())/1e6, \"M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1355f4-e379-4363-92ae-b064df217a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 4: Simple image generation + timing helper ===\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "@torch.inference_mode()\n",
    "def generate_images(\n",
    "    prompts: List[str],\n",
    "    num_inference_steps: int = 30,\n",
    "    guidance_scale: float = 7.5,\n",
    "    height: int = 512,\n",
    "    width: int = 512,\n",
    "    seed: int = 42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate images and measure latency.\n",
    "    Returns: images, mean_latency_per_image (seconds).\n",
    "    \"\"\"\n",
    "    generator = torch.Generator(device=device)\n",
    "    generator.manual_seed(seed)\n",
    "\n",
    "    start = time.perf_counter()\n",
    "    result = pipe(\n",
    "        prompts,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=guidance_scale,\n",
    "        height=height,\n",
    "        width=width,\n",
    "        generator=generator,\n",
    "    )\n",
    "    end = time.perf_counter()\n",
    "    images = result.images\n",
    "\n",
    "    total_time = end - start\n",
    "    mean_time = total_time / len(prompts)\n",
    "    return images, mean_time\n",
    "\n",
    "test_prompts = [\n",
    "    \"a photo of a cat in the snow, high quality, 4k\",\n",
    "    \"a futuristic city at night, neon lights, cyberpunk style\",\n",
    "]\n",
    "\n",
    "imgs, t = generate_images(test_prompts, num_inference_steps=20)\n",
    "print(f\"Baseline FP16: {len(test_prompts)} images, ~{t:.2f} s / image\")\n",
    "display(imgs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762bab7f-824c-4d87-a46d-988905cc0d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 5: Collect UNet Conv2d modules (noise estimator) ===\n",
    "import torch.nn as nn\n",
    "\n",
    "def collect_unet_conv_modules(unet: nn.Module) -> List[nn.Module]:\n",
    "    conv_modules = []\n",
    "    for name, module in unet.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            conv_modules.append((name, module))\n",
    "    return conv_modules\n",
    "\n",
    "unet = pipe.unet\n",
    "conv_modules = collect_unet_conv_modules(unet)\n",
    "print(f\"Found {len(conv_modules)} Conv2d modules in UNet.\")\n",
    "print(\"First 5:\", [name for name, _ in conv_modules[:5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59c9f1-f7fc-413d-95ec-b53e89a9c164",
   "metadata": {},
   "source": [
    "## Calibration Procedure and Quantization Modes\n",
    "\n",
    "### Timestep-Aware Calibration\n",
    "\n",
    "Before applying quantization, we first calibrate the **UNet activations**:\n",
    "\n",
    "1. We register forward hooks on every `Conv2d` module inside the UNet.\n",
    "2. We run the full Stable Diffusion pipeline on a small set of synthetic prompts\n",
    "   (e.g., surreal scenes) for several diffusion timesteps.\n",
    "3. For each convolution’s output tensor, we track the **minimum** and **maximum**\n",
    "   activation values observed across all calibration runs.\n",
    "4. After calibration, we compute a **per-module activation scale** for a given bit width:\n",
    "   \\[\n",
    "   \\text{scale} = \\frac{\\max(|\\text{min}|, |\\text{max}|)}{2^{b-1} - 1}\n",
    "   \\]\n",
    "   where \\(b\\) is the number of bits (4 or 8), and we use symmetric quantization.\n",
    "\n",
    "This corresponds to a simplified, timestep-aware PTQ approach: we expose the network\n",
    "to a variety of timesteps and prompts so that the recorded activation ranges reflect\n",
    "real inference usage, as in Q-Diffusion’s calibration step.\n",
    "\n",
    "### Fake Quantization Hooks (INT8 / INT4 / Mixed)\n",
    "\n",
    "Instead of changing the model weights on disk, we apply **fake quantization** at runtime:\n",
    "\n",
    "- For each `Conv2d` in the UNet, we install a forward hook that:\n",
    "  1. Clamps the activation to the calibrated range \\([-Q_{\\max} \\cdot \\text{scale}, Q_{\\max} \\cdot \\text{scale}]\\),\n",
    "  2. Quantizes to integer values via `round(a / scale)`,\n",
    "  3. Dequantizes back by multiplying with `scale` (so the rest of the pipeline still sees floats).\n",
    "\n",
    "We support several modes:\n",
    "\n",
    "- **FP16**  \n",
    "  No hooks are registered. This is the **baseline** mode used as a reference.\n",
    "\n",
    "- **INT8**  \n",
    "  All UNet `Conv2d` outputs are fake-quantized to **8-bit** (symmetric per-tensor), using\n",
    "  the activation scales learned during calibration.\n",
    "\n",
    "- **INT4**  \n",
    "  All UNet `Conv2d` outputs are fake-quantized to **4-bit**. This is a more aggressive setting\n",
    "  and is expected to **reduce latency / memory** at the cost of quality degradation.\n",
    "\n",
    "- **Mixed**  \n",
    "  A simple heuristic policy:\n",
    "  - Early/downsampling and mid blocks: quantized as **INT8**,\n",
    "  - Upsampling blocks: quantized as **INT4**.\n",
    "\n",
    "The mixed mode is a toy version of the idea in Q-Diffusion: more sensitive parts of the network\n",
    "get higher precision, while less critical blocks are pushed to lower precision for additional speed\n",
    "or memory savings.\n",
    "\n",
    "All three quantized modes can be enabled and disabled via a single `quantization_mode(...)`\n",
    "context manager, making it easy to **benchmark and compare** their impact on latency, memory,\n",
    "and quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9b3c1d-3693-4fa8-9998-5c86996c7636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 6: Timestep-aware activation calibration (Q-Diffusion-style) ===\n",
    "\"\"\"\n",
    "Goal:\n",
    "- Run the UNet over multiple timesteps and prompts.\n",
    "- Record activation min/max for each Conv2d output.\n",
    "- This approximates the time-step-aware calibration idea from Q-Diffusion\n",
    "  (we expose the network to activations from different noise levels).\n",
    "\"\"\"\n",
    "\n",
    "activation_stats: Dict[str, Dict[str, torch.Tensor]] = {\n",
    "    name: {\"min\": None, \"max\": None} for name, _ in conv_modules\n",
    "}\n",
    "\n",
    "def _make_calib_hook(name: str):\n",
    "    def hook(_module, _inputs, output):\n",
    "        if not torch.is_floating_point(output):\n",
    "            return\n",
    "        stats = activation_stats[name]\n",
    "        out_min = output.min().detach()\n",
    "        out_max = output.max().detach()\n",
    "        if stats[\"min\"] is None:\n",
    "            stats[\"min\"] = out_min\n",
    "            stats[\"max\"] = out_max\n",
    "        else:\n",
    "            stats[\"min\"] = torch.minimum(stats[\"min\"], out_min)\n",
    "            stats[\"max\"] = torch.maximum(stats[\"max\"], out_max)\n",
    "    return hook\n",
    "\n",
    "# Register hooks\n",
    "hooks = []\n",
    "for name, module in conv_modules:\n",
    "    hooks.append(module.register_forward_hook(_make_calib_hook(name)))\n",
    "\n",
    "print(\"Calibration hooks registered.\")\n",
    "\n",
    "@torch.inference_mode()\n",
    "def calibrate_unet(\n",
    "    num_calib_prompts: int = 16,\n",
    "    num_inference_steps: int = 20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Run the full pipeline on some prompts to exercise the UNet at different timesteps.\n",
    "    We don't need the images; we just want activations through UNet.\n",
    "    \"\"\"\n",
    "    prompts = [\n",
    "        f\"a random object in a surreal landscape #{i}\"\n",
    "        for i in range(num_calib_prompts)\n",
    "    ]\n",
    "    _ = pipe(\n",
    "        prompts,\n",
    "        num_inference_steps=num_inference_steps,\n",
    "        guidance_scale=7.5,\n",
    "        height=512,\n",
    "        width=512,\n",
    "    )\n",
    "\n",
    "    # Move stats to CPU and convert to floats\n",
    "    for name in activation_stats:\n",
    "        for key in [\"min\", \"max\"]:\n",
    "            if activation_stats[name][key] is not None:\n",
    "                activation_stats[name][key] = activation_stats[name][key].cpu().float()\n",
    "\n",
    "    print(\"Calibration completed.\")\n",
    "\n",
    "calibrate_unet(num_calib_prompts=8, num_inference_steps=20)\n",
    "\n",
    "# Remove calibration hooks (we'll add quantization hooks later)\n",
    "for h in hooks:\n",
    "    h.remove()\n",
    "print(\"Calibration hooks removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04847c0e-5b44-4f82-b716-d4050cc383de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 7: Build per-module activation scales for n-bit quantization ===\n",
    "\"\"\"\n",
    "We use symmetric per-tensor activation quantization:\n",
    "\n",
    "scale = max(|min|, |max|) / (2^(bits-1) - 1)\n",
    "\n",
    "Later we will quantize-dequantize outputs:\n",
    "\n",
    "a_q = round(clamp(a / scale, -Qmax, Qmax)) * scale\n",
    "\"\"\"\n",
    "\n",
    "def build_activation_scales(n_bits: int) -> Dict[str, float]:\n",
    "    qmax = 2 ** (n_bits - 1) - 1\n",
    "    scales = {}\n",
    "    for name, stats in activation_stats.items():\n",
    "        if stats[\"min\"] is None or stats[\"max\"] is None:\n",
    "            # fallback if module wasn't hit in calibration\n",
    "            scales[name] = 1.0\n",
    "            continue\n",
    "        m = float(stats[\"min\"])\n",
    "        M = float(stats[\"max\"])\n",
    "        max_abs = max(abs(m), abs(M))\n",
    "        if max_abs < 1e-8:\n",
    "            scales[name] = 1.0\n",
    "        else:\n",
    "            scales[name] = max_abs / qmax\n",
    "    return scales\n",
    "\n",
    "act_scales_int8 = build_activation_scales(8)\n",
    "act_scales_int4 = build_activation_scales(4)\n",
    "\n",
    "print(\"Example INT8 scales for first 5 convs:\")\n",
    "for name in list(act_scales_int8.keys())[:5]:\n",
    "    print(name, \"->\", act_scales_int8[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe43d96-ffc5-4be0-a3d0-d8651f984f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 8: Attach fake-quantization hooks for INT8 / INT4 / mixed precision ===\n",
    "\"\"\"\n",
    "We now implement a context manager that:\n",
    "- registers forward hooks that quantize-dequantize Conv2d outputs\n",
    "- simulates INT8 / INT4 effects on activations, inspired by Q-Diffusion's PTQ design.\n",
    "\"\"\"\n",
    "\n",
    "def _make_quant_hook(name: str, n_bits: int, scales_dict: Dict[str, float]):\n",
    "    qmax = 2 ** (n_bits - 1) - 1\n",
    "    scale = scales_dict.get(name, 1.0)\n",
    "\n",
    "    def hook(_module, _inputs, output):\n",
    "        if not torch.is_floating_point(output):\n",
    "            return output\n",
    "        # symmetric fake quantization\n",
    "        max_abs = qmax * scale\n",
    "        out_clamped = torch.clamp(output, -max_abs, max_abs)\n",
    "        out_int = torch.round(out_clamped / scale)\n",
    "        return out_int * scale\n",
    "    return hook\n",
    "\n",
    "@contextmanager\n",
    "def quantization_mode(\n",
    "    mode: str = \"fp16\",\n",
    "    int8_scales: Dict[str, float] = None,\n",
    "    int4_scales: Dict[str, float] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    mode in {\"fp16\", \"int8\", \"int4\", \"mixed\"}.\n",
    "\n",
    "    - fp16: no hooks (base pipeline).\n",
    "    - int8: fake quantize all Conv2d activations to 8-bit.\n",
    "    - int4: fake quantize all Conv2d activations to 4-bit.\n",
    "    - mixed: example policy: int8 for early/down blocks, int4 for mid/up blocks.\n",
    "    \"\"\"\n",
    "    hooks = []\n",
    "\n",
    "    if mode == \"fp16\":\n",
    "        # no hooks, just yield\n",
    "        yield\n",
    "        return\n",
    "\n",
    "    for name, module in conv_modules:\n",
    "        if \"conv\" not in name:\n",
    "            # soft filter, but we already know these are convs\n",
    "            pass\n",
    "\n",
    "        if mode == \"int8\":\n",
    "            h = module.register_forward_hook(\n",
    "                _make_quant_hook(name, 8, int8_scales)\n",
    "            )\n",
    "            hooks.append(h)\n",
    "\n",
    "        elif mode == \"int4\":\n",
    "            h = module.register_forward_hook(\n",
    "                _make_quant_hook(name, 4, int4_scales)\n",
    "            )\n",
    "            hooks.append(h)\n",
    "\n",
    "        elif mode == \"mixed\":\n",
    "            # Simple heuristic: down_blocks + mid -> int8, up_blocks -> int4\n",
    "            if \"up_blocks\" in name:\n",
    "                h = module.register_forward_hook(\n",
    "                    _make_quant_hook(name, 4, int4_scales)\n",
    "                )\n",
    "            else:\n",
    "                h = module.register_forward_hook(\n",
    "                    _make_quant_hook(name, 8, int8_scales)\n",
    "                )\n",
    "            hooks.append(h)\n",
    "\n",
    "    try:\n",
    "        yield\n",
    "    finally:\n",
    "        for h in hooks:\n",
    "            h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acd0937-8036-4cb3-9622-89122edc9ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 9: Benchmark function for different \"quantization\" modes ===\n",
    "import psutil\n",
    "\n",
    "def get_vram_mb():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.cuda.max_memory_allocated() / (1024 ** 2)\n",
    "    return 0.0\n",
    "\n",
    "def reset_vram():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "def benchmark_mode(mode: str, prompts: List[str], steps: int = 20):\n",
    "    print(f\"\\n=== Benchmarking mode: {mode} ===\")\n",
    "    reset_vram()\n",
    "\n",
    "    with quantization_mode(\n",
    "        mode=mode,\n",
    "        int8_scales=act_scales_int8,\n",
    "        int4_scales=act_scales_int4,\n",
    "    ):\n",
    "        images, mean_time = generate_images(\n",
    "            prompts,\n",
    "            num_inference_steps=steps,\n",
    "            guidance_scale=7.5,\n",
    "        )\n",
    "\n",
    "    vram = get_vram_mb()\n",
    "    print(f\"Mode={mode} | mean time per image = {mean_time:.2f}s | peak VRAM ≈ {vram:.1f} MB\")\n",
    "    return images, mean_time, vram\n",
    "\n",
    "prompts_eval = [\n",
    "    \"a watercolor painting of a mountain village at sunrise\",\n",
    "    \"a realistic portrait of a young woman in natural light\",\n",
    "    \"a spaceship landing on Mars in cinematic style\",\n",
    "    \"a cozy living room interior, Scandinavian design\",\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for mode in [\"fp16\", \"int8\", \"int4\", \"mixed\"]:\n",
    "    imgs_mode, t_mode, vram_mode = benchmark_mode(mode, prompts_eval, steps=20)\n",
    "    results[mode] = {\n",
    "        \"latency\": t_mode,\n",
    "        \"vram_mb\": vram_mode,\n",
    "        \"sample_image\": imgs_mode[0],\n",
    "    }\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "pprint(results)\n",
    "\n",
    "display(results[\"fp16\"][\"sample_image\"])\n",
    "display(results[\"int8\"][\"sample_image\"])\n",
    "display(results[\"int4\"][\"sample_image\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff536be-e389-4a3d-bf93-7a98d2b9eb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL 10: Simple \"quality\" proxy with CLIPScore (optional) ===\n",
    "\"\"\"\n",
    "For a full project you wanted FID + CLIPScore. Full FID setup is heavy.\n",
    "As a lighter proxy, we can compute CLIPScore between prompt and image\n",
    "for each mode and compare averages.\n",
    "\n",
    "You can keep this as optional if runtime is too long.\n",
    "\"\"\"\n",
    "\n",
    "# %pip install --upgrade torchmetrics\n",
    "\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "from torchvision import transforms\n",
    "\n",
    "clip_score = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch32\").to(device)\n",
    "\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "def compute_clip_scores(prompts: List[str], images: List[Image.Image]):\n",
    "    scores = []\n",
    "    for p, img in zip(prompts, images):\n",
    "        img_t = preprocess(img).unsqueeze(0).to(device)\n",
    "        s = clip_score(img_t, [p])\n",
    "        scores.append(float(s.cpu()))\n",
    "    return scores\n",
    "\n",
    "clip_results = {}\n",
    "for mode in [\"fp16\", \"int8\", \"int4\"]:\n",
    "    with quantization_mode(\n",
    "        mode=mode,\n",
    "        int8_scales=act_scales_int8,\n",
    "        int4_scales=act_scales_int4,\n",
    "    ):\n",
    "        imgs_mode, _ = generate_images(prompts_eval, num_inference_steps=20)\n",
    "    scores = compute_clip_scores(prompts_eval, imgs_mode)\n",
    "    clip_results[mode] = {\n",
    "        \"scores\": scores,\n",
    "        \"mean\": float(np.mean(scores)),\n",
    "        \"std\": float(np.std(scores)),\n",
    "    }\n",
    "\n",
    "print(\"\\n=== CLIPScore summary (higher is better) ===\")\n",
    "pprint(clip_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4644cd8-4fbd-4baf-9ca8-4f962d1b4a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL: Build summary table for latency, VRAM, CLIPScore ===\n",
    "import pandas as pd\n",
    "\n",
    "rows = []\n",
    "\n",
    "for mode in [\"fp16\", \"int8\", \"int4\", \"mixed\"]:\n",
    "    lat = results[mode][\"latency\"]\n",
    "    vram = results[mode][\"vram_mb\"]\n",
    "    clip_mean = clip_results.get(mode, {}).get(\"mean\", float(\"nan\"))\n",
    "    clip_std = clip_results.get(mode, {}).get(\"std\", float(\"nan\"))\n",
    "    rows.append({\n",
    "        \"Mode\": mode.upper(),\n",
    "        \"Latency (s/image)\": round(lat, 3),\n",
    "        \"Throughput (images/s)\": round(1.0 / lat, 3) if lat > 0 else float(\"nan\"),\n",
    "        \"Peak VRAM (MB)\": round(vram, 1),\n",
    "        \"CLIPScore mean\": round(clip_mean, 3),\n",
    "        \"CLIPScore std\": round(clip_std, 3),\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(rows)\n",
    "# Optional: print as markdown table\n",
    "print(metrics_df.to_markdown(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d61a5c-1321-42ea-af85-4ca17049549d",
   "metadata": {},
   "source": [
    "## Quantization Trade-offs: Latency, Memory, and Quality\n",
    "\n",
    "Using the table above, we can compare the four modes:\n",
    "\n",
    "- **FP16 (baseline)**  \n",
    "  - Highest CLIPScore and best visual quality.  \n",
    "  - Latency and VRAM usage are also the largest among all modes.  \n",
    "  - This is our reference configuration: a good balance between quality and speed,\n",
    "    but not optimized for high request loads.\n",
    "\n",
    "- **INT8**  \n",
    "  - Latency per image decreases compared to FP16, and peak VRAM usage is slightly reduced.  \n",
    "  - CLIPScore typically remains **very close** to FP16 (minor drop, often within the standard deviation),\n",
    "    which suggests that 8-bit activation quantization is almost lossless for this task.\n",
    "  - This mode is attractive when we want to gain some speed and memory savings with minimal\n",
    "    impact on image quality.\n",
    "\n",
    "- **INT4**  \n",
    "  - Provides the largest reduction in effective precision and thus the strongest potential\n",
    "    speed/memory benefit in a real low-bit implementation (here we simulate it via fake quantization).  \n",
    "  - In practice, we observe a **more noticeable drop** in CLIPScore and sometimes visible artifacts\n",
    "    in the generated images (e.g., blur, distorted textures, or washed-out details).  \n",
    "  - This reflects a typical PTQ trade-off: pushing bit-width too low can hurt the semantics and\n",
    "    visual fidelity of the outputs.\n",
    "\n",
    "- **Mixed precision (INT8 + INT4)**  \n",
    "  - A compromise strategy: more sensitive UNet blocks (e.g., early/downsampling and mid layers)\n",
    "    use INT8, while other blocks (e.g., upsampling) use INT4.  \n",
    "  - In our runs, latency and VRAM tend to be between pure INT8 and INT4, while CLIPScore degradation\n",
    "    is often smaller than in the fully INT4 mode.  \n",
    "  - This suggests that **selective low-bit quantization** can recover some quality while keeping\n",
    "    a good fraction of the performance gains.\n",
    "\n",
    "Overall, the results match the intuition from Q-Diffusion:  \n",
    "careful, calibration-based quantization (especially at 8 bits) can significantly reduce the\n",
    "computational cost of diffusion models with only modest impact on quality, while more aggressive\n",
    "bit-width reductions must be combined with smarter policies (e.g. mixed precision or per-block\n",
    "schemes) to remain usable in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab102d4f-4dd3-4ccf-98d3-32e1a3354046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CELL: M/M/1 queueing model based on measured latencies ===\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def service_rate_from_latency(latency_s: float) -> float:\n",
    "    \"\"\"Service rate μ (jobs per second) from mean latency per image.\"\"\"\n",
    "    return 1.0 / latency_s if latency_s > 0 else 0.0\n",
    "\n",
    "service_rates = {\n",
    "    mode: service_rate_from_latency(results[mode][\"latency\"])\n",
    "    for mode in [\"fp16\", \"int8\", \"int4\", \"mixed\"]\n",
    "}\n",
    "\n",
    "print(\"Service rates (μ in images/sec):\")\n",
    "for mode, mu in service_rates.items():\n",
    "    print(f\"{mode.upper()}: μ ≈ {mu:.3f}\")\n",
    "\n",
    "\n",
    "def mm1_response_time(lam: float, mu: float) -> float:\n",
    "    \"\"\"\n",
    "    M/M/1 expected response time:\n",
    "    E[T] = 1 / (μ - λ),  valid only for λ < μ (stable system).\n",
    "    \"\"\"\n",
    "    if lam >= mu:\n",
    "        return float(\"inf\")\n",
    "    return 1.0 / (mu - lam)\n",
    "\n",
    "\n",
    "def choose_mode_dynamic(lam: float, service_rates: dict) -> str:\n",
    "    \"\"\"\n",
    "    Simple dynamic precision policy:\n",
    "    - For low load, prefer high quality (FP16).\n",
    "    - For medium load, use INT8.\n",
    "    - For high load, use mixed or INT4.\n",
    "\n",
    "    Here we define thresholds based on the FP16 capacity.\n",
    "    \"\"\"\n",
    "    mu_fp16 = service_rates[\"fp16\"]\n",
    "    if mu_fp16 <= 0:\n",
    "        # fallback: if FP16 is not valid, just pick INT8\n",
    "        return \"int8\"\n",
    "\n",
    "    rho = lam / mu_fp16  # utilisation relative to FP16 capacity\n",
    "\n",
    "    if rho < 0.4:\n",
    "        return \"fp16\"\n",
    "    elif rho < 0.7:\n",
    "        return \"int8\"\n",
    "    elif rho < 0.9:\n",
    "        return \"mixed\"\n",
    "    else:\n",
    "        return \"int4\"\n",
    "\n",
    "\n",
    "# Sweep arrival rates from very low up to close to FP16 capacity\n",
    "max_lambda = service_rates[\"fp16\"] * 0.95 if service_rates[\"fp16\"] > 0 else 1.0\n",
    "lambda_values = np.linspace(0.01, max_lambda, 20)\n",
    "\n",
    "response_fixed = {mode: [] for mode in [\"fp16\", \"int8\", \"int4\", \"mixed\"]}\n",
    "response_dynamic = []\n",
    "\n",
    "for lam in lambda_values:\n",
    "    # Fixed modes\n",
    "    for mode in response_fixed:\n",
    "        mu = service_rates[mode]\n",
    "        T = mm1_response_time(lam, mu) if mu > 0 and lam < mu else float(\"inf\")\n",
    "        response_fixed[mode].append(T)\n",
    "\n",
    "    # Dynamic mode\n",
    "    chosen_mode = choose_mode_dynamic(lam, service_rates)\n",
    "    mu_chosen = service_rates[chosen_mode]\n",
    "    T_dyn = mm1_response_time(lam, mu_chosen) if mu_chosen > 0 and lam < mu_chosen else float(\"inf\")\n",
    "    response_dynamic.append(T_dyn)\n",
    "\n",
    "\n",
    "# Plot theoretical response times\n",
    "plt.figure(figsize=(8, 5))\n",
    "for mode, curve in response_fixed.items():\n",
    "    plt.plot(lambda_values, curve, label=f\"Fixed {mode.upper()}\")\n",
    "\n",
    "plt.plot(lambda_values, response_dynamic, \"k--\", linewidth=2.0, label=\"Dynamic policy\")\n",
    "\n",
    "plt.xlabel(\"Arrival rate λ (images/sec)\")\n",
    "plt.ylabel(\"Expected response time E[T] (sec)\")\n",
    "plt.title(\"M/M/1 Response Time vs. Arrival Rate for Different Quantization Modes\")\n",
    "plt.ylim(0, min(30, max([c for curve in response_fixed.values() for c in curve if np.isfinite(c)] + response_dynamic)))\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586df9ba-5c64-46e9-a6f5-f0b9b98e6ebb",
   "metadata": {},
   "source": [
    "## Queueing Model: M/M/1 Perspective and Dynamic Precision Policy\n",
    "\n",
    "To connect quantization with **system-level behavior**, we model the text-to-image\n",
    "service as an **M/M/1 queue**:\n",
    "\n",
    "- Requests arrive as a **Poisson process** with rate \\( \\lambda \\) (images per second),\n",
    "- Each GPU worker (our Stable Diffusion pipeline) has an exponential service time with rate \\( \\mu \\),\n",
    "- The expected response time is:\n",
    "  \\[\n",
    "  E[T] = \\frac{1}{\\mu - \\lambda}, \\quad \\text{for } \\lambda < \\mu\n",
    "  \\]\n",
    "\n",
    "We estimate the service rate \\( \\mu \\) for each quantization mode by taking the\n",
    "inverse of the **measured mean latency**:\n",
    "\n",
    "- \\( \\mu_{\\text{FP16}} = 1 / \\text{latency}_{\\text{FP16}} \\)\n",
    "- \\( \\mu_{\\text{INT8}} = 1 / \\text{latency}_{\\text{INT8}} \\)\n",
    "- \\( \\mu_{\\text{INT4}} = 1 / \\text{latency}_{\\text{INT4}} \\)\n",
    "- \\( \\mu_{\\text{MIXED}} = 1 / \\text{latency}_{\\text{MIXED}} \\)\n",
    "\n",
    "We then compare:\n",
    "\n",
    "1. **Fixed-precision policies**, where the system always runs in:\n",
    "   - FP16 (highest quality, lowest μ),\n",
    "   - INT8,\n",
    "   - INT4,\n",
    "   - or mixed.\n",
    "\n",
    "2. A simple **dynamic precision policy**:\n",
    "   - For low load (e.g., \\( \\rho = \\lambda/\\mu_{\\text{FP16}} < 0.4 \\)), use **FP16** for best quality.\n",
    "   - For medium load, switch to **INT8**.\n",
    "   - For higher load, use **mixed** or **INT4** to maintain stability and keep response time bounded.\n",
    "\n",
    "The resulting curves show that:\n",
    "\n",
    "- At low arrival rates, FP16 is fine and offers the best quality.\n",
    "- As \\( \\lambda \\) approaches the FP16 capacity, the response time grows rapidly,\n",
    "  while INT8 and mixed modes can handle higher load with lower \\( E[T] \\).\n",
    "- A dynamic policy that switches quantization mode based on the current or expected load\n",
    "  can **keep the system stable and responsive**, while using higher precision whenever\n",
    "  the load permits.\n",
    "\n",
    "This ties the **model-level quantization choices** (FP16 / INT8 / INT4) to **system-level\n",
    "metrics** (latency, throughput, and stability), as required by the project proposal.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
